[
  {
    "display_name": "Spark Script Engine",
    "form_info": "Spark job that lets the user enter execute a scala script in the Spark context",
    "expandable": true,
    "child_filters": [
      "analytic_input",
      "analytic_output",
      "enrichment_meta",
      "batch_enrichment_meta",
      "batch_technology_override"
    ],
    "key": "analytic_job",
    "categories": [
      "Spark Processing"
    ],
    "filters": [
      "Bucket/**"
    ],
    "schema": [
      {
        "templateOptions": {
          "required": true,
          "pattern": "[a-zA-Z0-9_]+",
          "label": "Unique Job Name",
          "placeholder": "A Short Name For This Job (Alphanumeric/_ only, no spaces - used for dependencies etc)"
        },
        "key": "_short_name",
        "type": "horizontalInput"
      },
      {
        "templateOptions": {
          "required": true,
          "label": "Analytic Type",
          "options": [
            {
              "name": "Batch",
              "value": "batch"
            },
            {
              "name": "Streaming",
              "value": "stream"
            }
          ]
        },
        "type": "horizontalSelect",
        "key": "analytic_type"
      },
      {
        "templateOptions": {
          "required": false,
          "label": "Dependencies",
          "placeholder": "A comma-separated list of dependencies (other job names) for this job"
        },
        "key": "dependencies",
        "type": "horizontalInput"
      },
      {
        "template": "<hr/>"
      },
      {
        "key": "script",
        "type": "code_input",
        "templateOptions": {
          "codemirror": {
            "lineNumbers": true,
            "smartIndent": true,
            "mode": "clike"
          },
          "label": "Scala Spark Script",
          "required": false
        }
      },
      {
        "template": "<hr/>"
      },
      {
        "type": "multiInput",
        "templateOptions": {
          "inputOptions": {
            "type": "input",
            "templateOptions": {
              "placeholder": "Add jar files to the scala path from the shared library"
            }
          },
          "label": "JAR libraries"
        },
        "key": "library_names_or_ids"
      },
      {
        "type": "multiInput",
        "templateOptions": {
          "inputOptions": {
            "type": "input",
            "templateOptions": {
              "placeholder": "Provide the scala script with miscellaneous files from the shared library"
            }
          },
          "label": "Misc files"
        },
        "key": "uploaded_files"
      },
      {
        "template": "<hr/>"
      },
      {
        "templateOptions": {
          "label": "Show advanced options",
          "placeholder": ""
        },
        "type": "checkbox",
        "key": "advanced_options"
      },
      {
        "hideExpression": "!model.advanced_options",
        "fieldGroup": [
          {
            "key": "job_config",
            "type": "code_input",
            "templateOptions": {
              "codemirror": {
                "lineNumbers": true,
                "smartIndent": true,
                "mode": "javascript"
              },
              "label": "Misc JSON Config"
            }
          },
          {
            "templateOptions": {
              "label": "Insert Job Config into the Spark Config options",
              "placeholder": "If true then can access this JSON as a string in the pure Spark Conf object"
            },
            "type": "checkbox",
            "key": "include_job_config_in_spark_config"
          },
          {
            "templateOptions": {
              "required": false,
              "label": "Analytic Technology Override",
              "placeholder": "To use a non-standard JAR for this job (eg for testing)"
            },
            "key": "analytic_technology_name_or_id",
            "type": "horizontalInput"
          },
          {
            "templateOptions": {
              "required": false,
              "label": "Entry Point Override",
              "placeholder": "To use a non-standard entry point for this job (eg for testing)"
            },
            "key": "entry_point",
            "type": "horizontalInput"
          },
          {
            "templateOptions": {
              "required": false,
              "label": "Module Name Override",
              "placeholder": "To use a non-standard JAR for this job (eg for testing)"
            },
            "key": "module_name_or_id",
            "type": "horizontalInput"
          },
          {
            "templateOptions": {
              "required": false,
              "label": "Node List Rules",
              "placeholder": "A list of (short) hostnames on which this job can run"
            },
            "key": "node_list_rules",
            "type": "horizontalInput"
          },
          {
            "className": "section-label",
            "template": "<hr /><div><strong>Spark Parameters</strong></div>"
          },
          {
            "templateOptions": {
              "btnText": "Add",
              "fields": [
                {
                  "className": "row",
                  "fieldGroup": [
                    {
                      "templateOptions": {
                        "required": true,
                        "label": "Parameter Key",
                        "placeholder": "Must start with \"spark.\" eg \"spark.config.param\""
                      },
                      "key": "key",
                      "type": "input",
                      "className": "col-xs-6"
                    },
                    {
                      "templateOptions": {
                        "required": true,
                        "label": "Parameter Value",
                        "placeholder": "The value"
                      },
                      "key": "value",
                      "type": "input",
                      "className": "col-xs-6"
                    }
                  ]
                }
              ]
            },
            "type": "repeatSection",
            "key": "spark_config"
          },
          {
            "className": "section-label",
            "template": "<hr /><div><strong>Spark Executor Command Line Options</strong></div>"
          },
          {
            "templateOptions": {
              "btnText": "Add",
              "fields": [
                {
                  "className": "row",
                  "fieldGroup": [
                    {
                      "templateOptions": {
                        "required": true,
                        "label": "Command line switch",
                        "placeholder": "eg --conf"
                      },
                      "key": "key",
                      "type": "input",
                      "className": "col-xs-6"
                    },
                    {
                      "templateOptions": {
                        "required": true,
                        "label": "Command line value",
                        "placeholder": "The value associated with the switch"
                      },
                      "key": "value",
                      "type": "input",
                      "className": "col-xs-6"
                    }
                  ]
                }
              ]
            },
            "type": "repeatSection",
            "key": "system_config"
          }
        ]
      },
      {
        "template": "<hr/>"
      },
      {
        "templateOptions": {
          "label": "Additional library options",
          "placeholder": "Show more file/lib related options"
        },
        "type": "checkbox",
        "key": "more_files"
      },
      {
        "hideExpression": "!model.more_files",
        "fieldGroup": [
          {
            "type": "multiInput",
            "templateOptions": {
              "inputOptions": {
                "type": "input",
                "templateOptions": {
                  "placeholder": "Add other JARs from the local file system"
                }
              },
              "label": "External JAR libraries"
            },
            "key": "external_jars"
          },
          {
            "type": "multiInput",
            "templateOptions": {
              "inputOptions": {
                "type": "input",
                "templateOptions": {
                  "placeholder": "Provide the scala script with miscellaneous files from the local file system"
                }
              },
              "label": "External misc files"
            },
            "key": "external_files"
          }
        ]
      }
    ],
    "default_model": {
      "analytic_type": "batch",
      "lock_to_nodes": true,
      "analytic_technology_name_or_id": "/app/aleph2/library/spark_technology.jar",
      "job_config": "{\n}",
      "entry_point": "com.ikanow.aleph2.analytics.spark.assets.SparkScalaInterpreterTopology",
      "module_name_or_id": "/app/aleph2/library/spark_script_engine.jar",
      "cluster_mode": "yarn-cluster",
      "include_job_config_in_spark_config": true,
      "script": "// Like spark-shell, with the following built in args:\n// _a2.aleph2_context (IAnalyticsContext) - the Aleph2 context interface\n// _a2.inputs (Multimap<String, JavaPairRDD<Object, Tuple2<Long, IBatchRecord>>>) - a list of input (java) RDDs vs name\n// _a2.spark_context (SparkContext) / _a2.java_spark_context (JavaSparkContext) - Spark contexts\n// _a2.job_config (SparkTopologyConfigBean) - the full configuration object\n// Utility functions: _a2.emit(j: JsonNode), _a2.externalEmit(p: String, j: JsonNode), _a2.allInputs (union of the inputs)\n\n//eg:\nval logger = _a2.aleph2_context.getLogger(_a2.aleph2_context.getBucket());\nval inputs = _a2.allInputs();\nval count = inputs.rdd map { case (o, (l, record)) => record.getJson } map { json => _a2.emit(json) } count;\nlogger.log(Level.INFO, true, { () => \"Count = \" + count }, { () => \"SparkScriptEngine\" });\n"
    },
    "building_function": {
      "$fn": "function(errs, template, curr_obj, all_templates, root_obj, hierarchy, rows, cols) {\n  var new_obj = { inputs: [] };\n  new_obj.name = template.element.short_name;\n  new_obj.lock_to_nodes = template.element.form_model.lock_to_nodes;\n  new_obj.analytic_type = template.element.form_model.analytic_type;\n  new_obj.dependencies = a2_optional_array_from_csv(template.element.form_model.dependencies);\n  new_obj.analytic_technology_name_or_id = template.element.form_model.analytic_technology_name_or_id;\n  new_obj.module_name_or_id = template.element.form_model.module_name_or_id;\n  new_obj.library_names_or_ids = [];\n  if (null != template.element.form_model.library_names_or_ids) {\n    for (var i in template.element.form_model.library_names_or_ids) \n      new_obj.library_names_or_ids.push(template.element.form_model.library_names_or_ids[i]);\n  }\n  new_obj.node_list_rules = a2_optional_array_from_csv(template.element.form_model.node_list_rules);\n  new_obj.config = {};\n  \n  var config = new_obj.config;\n    \n  config.cluster_mode = template.element.form_model.cluster_mode;\n  config.enrich_pipeline = [];\n  config.language = template.element.form_model.language;\n  config.entry_point = template.element.form_model.entry_point;\n  config.script = template.element.form_model.script;\n\n  var spark_config = template.element.form_model.spark_config;\n  if (null != spark_config) {\n    config.spark_config = {};\n    for (var kv in spark_config) {\n      var el = spark_config[kv];\n      config.spark_config[el.key] = el.value;\n    }\n  }\n  var system_config = template.element.form_model.system_config;\n  if (null != system_config) {\n    config.system_config = {};\n    for (var kv in system_config) {\n      var el = system_config[kv];\n      config.system_config[el.key] = el.value;\n    }\n  }\n  if (template.element.form_model.job_config) config.job_config = JSON.parse(template.element.form_model.job_config);\n  config.include_job_config_in_spark_config = template.element.form_model.include_job_config_in_spark_config;\n  \n  config.uploaded_files = template.element.form_model.uploaded_files;\n  if (null != config.uploaded_files) {\n    if (null == new_obj.library_names_or_ids) new_obj.library_names_or_ids = [];\n    for (var x in config.uploaded_files) new_obj.library_names_or_ids.push(config.uploaded_files[x]);\n  }\n  config.uploaded_lang_files = template.element.form_model.uploaded_lang_files;\n  if (null != config.uploaded_lang_files) {\n    if (null == new_obj.library_names_or_ids) new_obj.library_names_or_ids = [];\n    for (var x in config.uploaded_lang_files) new_obj.library_names_or_ids.push(config.uploaded_lang_files[x]);\n  }\n  config.external_jars = template.element.form_model.external_jars;\n  config.external_files = template.element.form_model.external_files;\n  config.external_lang_files = template.element.form_model.external_lang_files;\n\n  curr_obj.jobs.push(new_obj);\n  return new_obj;\n}"
    },
    "post_building_function": {
      "$fn": "function(errs, template, curr_obj, all_templates, root_obj, hierarchy, rows, cols) {\n  var lib_map = {}; var curr_job = curr_obj.jobs[curr_obj.jobs.length-1]; \n  for ( var e in curr_job.config.enrich_pipeline ) { \n    if (curr_job.config.enrich_pipeline[e].module_name_or_id) {\n      lib_map[curr_job.config.enrich_pipeline[e].module_name_or_id] = true;\n    }\n    for ( l in curr_job.config.enrich_pipeline[e].library_names_or_ids ) { \n      lib_map[curr_job.config.enrich_pipeline[e].library_names_or_ids[l]] = true;\n    }\n  } \n  curr_job.library_names_or_ids = [];\n  var libs = (template.element.form_model.library_names_or_ids || []); \n  for (var kk in libs) curr_job.library_names_or_ids.push(libs[kk]); \n  for (var k in lib_map) curr_job.library_names_or_ids.push(k); \n}"
    },
    "validation_function": {
      "$fn": "function(errs, template, curr_obj, all_templates, root_obj, hierarchy, rows, cols) {\n    a2_must_edit(template, errs);\n}"
    },
    "post_validation_function": {
      "$fn": "function(errs, template, curr_obj, all_templates, root_obj, hierarchy, rows, cols) {\n  \n}"
    }
  }
]